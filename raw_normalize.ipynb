{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['BLCODE',\n",
    " 'SUB',\n",
    " 'AGE',\n",
    " 'SEX',\n",
    " 'YEAR',\n",
    " 'DAY',\n",
    " 'TIME',\n",
    " 'ASH',\n",
    " 'PRXY_TSA',\n",
    " 'TSA',\n",
    " 'MCI',\n",
    " 'VENT',\n",
    " 'AVM',\n",
    " 'COMF',\n",
    " 'ACT10',\n",
    " 'ACT20',\n",
    " 'ACT30',\n",
    " 'ACT60',\n",
    " 'MET',\n",
    " 'CLO',\n",
    " 'UPHOLST',\n",
    " 'INSUL',\n",
    " 'TA_H',\n",
    " 'TA_M',\n",
    " 'TA_L',\n",
    " 'DEWPT',\n",
    " 'PRTA-B',\n",
    " 'TG_H',\n",
    " 'TG_M',\n",
    " 'TG_L',\n",
    " 'VEL_H',\n",
    " 'VEL_M',\n",
    " 'VEL_L',\n",
    " 'TURB_H',\n",
    " 'TURB_M',\n",
    " 'TURB_L',\n",
    " 'TAAV',\n",
    " 'TRAV',\n",
    " 'TOP',\n",
    " 'VELAV',\n",
    " 'VELMAX',\n",
    " 'TUAV',\n",
    " 'PA',\n",
    " 'RH',\n",
    " 'ET',\n",
    " 'SET',\n",
    " 'TSENS',\n",
    " 'DISC',\n",
    " 'PMV',\n",
    " 'PPD',\n",
    " 'PD_H',\n",
    " 'PD_M',\n",
    " 'PD_L',\n",
    " 'PD_MAX',\n",
    " 'PCC',\n",
    " 'PCC_AG',\n",
    " 'PCS',\n",
    " 'PCEC1',\n",
    " 'PCEC2',\n",
    " 'PCEC3',\n",
    " 'PCEC4',\n",
    " 'PCEC5',\n",
    " 'PCEC6',\n",
    " 'PCEC7',\n",
    " 'PCED1',\n",
    " 'PCED2',\n",
    " 'PCED3',\n",
    " 'PCED4',\n",
    " 'PCED5',\n",
    " 'PCED6',\n",
    " 'PCED7',\n",
    " 'day15_ta',\n",
    " 'day06_ta',\n",
    " 'dayav_ta',\n",
    " 'day15_rh',\n",
    " 'day06_rh',\n",
    " 'dayav_rh',\n",
    " 'day15_et',\n",
    " 'day06_et',\n",
    " 'dayav_et']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('8_EXL.csv', sep=',', skiprows=6, skipfooter=4, engine='python', usecols=[i for i in range(80)], header=None, names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[columns_to_normalize] = data.replace('.', np.nan)[columns_to_normalize].apply(lambda x:(x-x.mean())/x.std())\n",
    "data.replace('.', np.nan)['ACT10'].astype(float).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for datafile in glob.glob('*_EXL.csv'):\n",
    "    datasets.append(pd.read_csv(datafile, sep=',', skiprows=6, skipfooter=4, engine='python', usecols=[i for i in range(80)], header=None, names=column_names))\n",
    "    \n",
    "data_all = pd.concat(datasets)\n",
    "select = data_all.apply(lambda r: any(['DIV' in str(e) or 'ACT10' in str(e) for e in r]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = data_all[~select].replace('.', np.nan).copy()\n",
    "data_raw.to_csv('raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_normalize = ['ACT10', 'ACT20', 'ACT30', 'ACT60', 'MET', 'CLO', 'UPHOLST', 'INSUL', 'TA_H', 'TA_M', 'TA_L', 'DEWPT', 'TG_M', 'VEL_M', 'TAAV', 'TRAV', 'TOP', 'VELAV', 'PA', 'RH', 'ET', 'SET', 'TSENS', 'DISC', 'PMV', 'PPD', 'PD_M', 'PD_MAX', 'PCC_AG', 'PCEC1', 'PCEC4', 'PCEC5', 'PCEC7', 'day15_ta', 'day06_ta', 'day15_rh', 'day06_rh', 'dayav_rh', 'day15_et', 'day06_et', 'dayav_et']\n",
    "                             \n",
    "clean_data = data_raw\n",
    "\n",
    "# mean and sigma normalize\n",
    "true_data = clean_data.copy()\n",
    "true_data[columns_to_normalize] = true_data.loc[:,columns_to_normalize].apply(lambda x:(x.astype(float)-x.astype(float).mean())/x.astype(float).std())\n",
    "true_data.to_csv('normalized_sigma.csv')\n",
    "\n",
    "# mean and max - min normalize\n",
    "maxmin_data = clean_data.copy()\n",
    "maxmin_data[columns_to_normalize] = maxmin_data.loc[:,columns_to_normalize].apply(lambda x:(x.astype(float)-x.astype(float).min())/(x.astype(float).max()-x.astype(float).min()))\n",
    "maxmin_data.to_csv('normalized_minmax.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
